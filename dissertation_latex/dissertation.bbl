\begin{thebibliography}{10}

\bibitem{angluin88_queries_concep_learn}
D.~Angluin.
\newblock Queries and concept learning.
\newblock {\em Machine learning}, 2(4):319--342, 1988.

\bibitem{aronszajn50_theor_reprod_kernel}
N.~Aronszajn.
\newblock Theory of reproducing kernels.
\newblock {\em Transactions of the American mathematical society},
  68(3):337--404, 1950.

\bibitem{bach13_sharp}
F.~Bach.
\newblock Sharp analysis of low-rank kernel matrix approximations.
\newblock In {\em Conference on Learning Theory}, pages 185--209, 2013.

\bibitem{bach12_equiv_between_herdin_condit_gradien_algor}
F.~Bach, S.~Lacoste-Julien, and G.~Obozinski.
\newblock On the equivalence between herding and conditional gradient
  algorithms.
\newblock {\em arXiv preprint arXiv:1203.4523}, 2012.

\bibitem{balcan10_true_sampl_compl_activ_learn}
M.-F. Balcan, S.~Hanneke, and J.~W. Vaughan.
\newblock The true sample complexity of active learning.
\newblock {\em Machine learning}, 80(2-3):111--139, 2010.

\bibitem{balkan15_activ_learn_moder_learn_theor}
M.-F. Balkan and R.~Urner.
\newblock Active learning - modern learning theory.
\newblock Active learning survey, January 2015.

\bibitem{baum92_query}
E.~B. Baum and K.~Lang.
\newblock Query learning can work poorly when a human oracle is used.
\newblock In {\em International joint conference on neural networks}, volume~8,
  page~8, 1992.

\bibitem{beck04_condit_gradien_method_with_linear}
A.~Beck and M.~Teboulle.
\newblock A conditional gradient method with linear rate of convergence for
  solving convex linear systems.
\newblock {\em Mathematical Methods of Operations Research}, 59(2):235--247,
  2004.

\bibitem{boyd04_convex}
S.~Boyd and L.~Vandenberghe.
\newblock {\em Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem{briol15_frank_wolfe_bayes}
F.-X. Briol, C.~Oates, M.~Girolami, and M.~A. Osborne.
\newblock Frank-wolfe bayesian quadrature: Probabilistic integration with
  theoretical guarantees.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1162--1170, 2015.

\bibitem{chaovalitwongse09_quadr_integ_progr}
W.~A. Chaovalitwongse, I.~P. Androulakis, and P.~M. Pardalos.
\newblock Quadratic integer programming: Complexity and equivalent forms.
\newblock {\em Encyclopedia of optimization}, pages 3153--3159, 2009.

\bibitem{chapelle09_semi_super_learn_o}
O.~Chapelle, B.~Scholkopf, and A.~Zien.
\newblock Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book
  reviews].
\newblock {\em IEEE Transactions on Neural Networks}, 20(3):542--542, 2009.

\bibitem{chattopadhyay13_batch_mode_activ_sampl_based}
R.~Chattopadhyay, Z.~Wang, W.~Fan, I.~Davidson, S.~Panchanathan, and J.~Ye.
\newblock Batch mode active sampling based on marginal probability distribution
  matching.
\newblock {\em ACM Transactions on Knowledge Discovery from Data (TKDD)},
  7(3):13, 2013.

\bibitem{chen12_super_sampl_from_kernel_herdin}
Y.~Chen, M.~Welling, and A.~Smola.
\newblock Super-samples from kernel herding.
\newblock {\em arXiv preprint arXiv:1203.3472}, 2012.

\bibitem{ciliberto18_advan_topic_machin_learn}
C.~Ciliberto.
\newblock Advanced topics in machine learning: Introduction to statistical
  learning theory.
\newblock \url{https://cciliber.github.io/intro-slt/}, 2018.

\bibitem{ciliberto16}
C.~Ciliberto, L.~Rosasco, and A.~Rudi.
\newblock A consistent regularization approach for structured prediction.
\newblock In {\em Advances in neural information processing systems}, pages
  4412--4420, 2016.

\bibitem{cohn94_improv_gener_with_activ_learn}
D.~Cohn, L.~Atlas, and R.~Ladner.
\newblock Improving generalization with active learning.
\newblock {\em Machine Learning}, 15(2):201--221, May 1994.

\bibitem{cohn96_activ_learn_with_statis_model}
D.~A. Cohn, Z.~Ghahramani, and M.~I. Jordan.
\newblock Active learning with statistical models.
\newblock {\em Journal of Artificial Intelligence Research}, 4(nil):129--145,
  1996.

\bibitem{cortes14_domain_adapt_sampl_bias_correc}
C.~Cortes and M.~Mohri.
\newblock Domain adaptation and sample bias correction theory and algorithm for
  regression.
\newblock {\em Theoretical Computer Science}, 519:103--126, 2014.

\bibitem{cortes19_adapt_based_gener_discr}
C.~Cortes, M.~Mohri, and A.~M. Medina.
\newblock Adaptation based on generalized discrepancy.
\newblock {\em Machine Learning Research, forthcoming. URL http://www. cs. nyu.
  edu/\~{} mohri/pub/daj. pdf}, 2019.

\bibitem{cressie90_origin_krigin}
N.~Cressie.
\newblock The origins of kriging.
\newblock {\em Mathematical geology}, 22(3):239--252, 1990.

\bibitem{dasgupta05_analy}
S.~Dasgupta.
\newblock Analysis of a greedy active learning strategy.
\newblock In {\em Advances in neural information processing systems}, pages
  337--344, 2005.

\bibitem{dasgupta06_coars}
S.~Dasgupta.
\newblock Coarse sample complexity bounds for active learning.
\newblock In {\em Advances in neural information processing systems}, pages
  235--242, 2006.

\bibitem{dasgupta08}
S.~Dasgupta, D.~J. Hsu, and C.~Monteleoni.
\newblock A general agnostic active learning algorithm.
\newblock In {\em Advances in neural information processing systems}, pages
  353--360, 2008.

\bibitem{desjardins19_how}
J.~Desjardins.
\newblock How much data is generated each day?
\newblock
  \url{https://www.weforum.org/agenda/2019/04/how-much-data-is-generated-each-day-cf4bddf29f},
  2019.
\newblock Accessed: 2019-07-29.

\bibitem{dua17_uci_machin_learn_repos}
D.~Dua and C.~Graff.
\newblock {UCI} machine learning repository, 2017.

\bibitem{evgeniou99_suppor}
T.~Evgeniou and M.~Pontil.
\newblock Support vector machines: Theory and applications.
\newblock In {\em Advanced Course on Artificial Intelligence}, pages 249--257.
  Springer, 1999.

\bibitem{fasshauer11_posit_defin_kernel}
G.~E. Fasshauer.
\newblock Positive definite kernels: Past, present and future.
\newblock {\em Dolomite Research Notes on Approximation}, 4:21--63, 2011.

\bibitem{fedorov10_optim_exper_desig}
V.~Fedorov.
\newblock Optimal experimental design.
\newblock {\em Wiley Interdisciplinary Reviews: Computational Statistics},
  2(5):581--589, 2010.

\bibitem{frank56_algor_quadr_progr}
M.~Frank and P.~Wolfe.
\newblock An algorithm for quadratic programming.
\newblock {\em Naval research logistics quarterly}, 3(1-2):95--110, 1956.

\bibitem{ganti12_upal}
R.~Ganti and A.~Gray.
\newblock Upal: Unbiased pool based active learning.
\newblock In {\em Artificial Intelligence and Statistics}, pages 422--431,
  2012.

\bibitem{gretton18_advan_topic_machin_learn}
A.~Gretton and D.~Seijdinovich.
\newblock Advanced topics in machine learning: Reproducing kernel hilbert
  spaces in machine learning.
\newblock
  \url{http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/rkhscourse.html}, 2018.

\bibitem{gretton09_covar_shift_by_kernel_mean_match}
A.~Gretton, A.~Smola, J.~Huang, M.~Schmittfull, K.~Borgwardt, and
  B.~Sch{\"o}lkopf.
\newblock Covariate shift by kernel mean matching.
\newblock {\em Dataset shift in machine learning}, 3(4):5, 2009.

\bibitem{gu12_towar}
Q.~Gu and J.~Han.
\newblock Towards active learning on graphs: An error bound minimization
  approach.
\newblock In {\em 2012 IEEE 12th International Conference on Data Mining},
  pages 882--887. IEEE, 2012.

\bibitem{guo08_discr}
Y.~Guo and D.~Schuurmans.
\newblock Discriminative batch mode active learning.
\newblock In {\em Advances in neural information processing systems}, pages
  593--600, 2008.

\bibitem{hoerl70_ridge_regres}
A.~E. Hoerl and R.~W. Kennard.
\newblock Ridge regression: Biased estimation for nonorthogonal problems.
\newblock {\em Technometrics}, 12(1):55--67, 1970.

\bibitem{jaggi13_revis_frank_wolfe}
M.~Jaggi.
\newblock Revisiting frank-wolfe: Projection-free sparse convex optimization.
\newblock In {\em ICML (1)}, pages 427--435, 2013.

\bibitem{krause08_near_optim_sensor_placem_gauss_proces}
A.~Krause, A.~Singh, and C.~Guestrin.
\newblock Near-optimal sensor placements in gaussian processes: Theory,
  efficient algorithms and empirical studies.
\newblock {\em Journal of Machine Learning Research}, 9(Feb):235--284, 2008.

\bibitem{lewis94}
D.~D. Lewis and W.~A. Gale.
\newblock A sequential algorithm for training text classifiers.
\newblock In {\em SIGIR'94}, pages 3--12. Springer, 1994.

\bibitem{manton15_primer_reprod_kernel_hilber_spaces}
J.~H. Manton, P.-O. Amblard, et~al.
\newblock A primer on reproducing kernel hilbert spaces.
\newblock {\em Foundations and Trends{\textregistered} in Signal Processing},
  8(1--2):1--126, 2015.

\bibitem{muandet17_kernel_mean_embed_distr}
K.~Muandet, K.~Fukumizu, B.~Sriperumbudur, B.~Sch{\"o}lkopf, et~al.
\newblock Kernel mean embedding of distributions: a review and beyond.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  10(1-2):1--141, 2017.

\bibitem{mueller97_integ_probab_metric_their_gener_class_funct}
A.~M{\"u}ller.
\newblock Integral probability metrics and their generating classes of
  functions.
\newblock {\em Advances in Applied Probability}, 29(2):429--443, 1997.

\bibitem{pak10_twitt}
A.~Pak and P.~Paroubek.
\newblock Twitter as a corpus for sentiment analysis and opinion mining.
\newblock In {\em LREc}, volume~10, pages 1320--1326, 2010.

\bibitem{qin14_proces_data_analy_era_big_data}
S.~J. Qin.
\newblock Process data analytics in the era of big data.
\newblock {\em AIChE Journal}, 60(9):3092--3100, 2014.

\bibitem{rudi18}
A.~Rudi, D.~Calandriello, L.~Carratino, and L.~Rosasco.
\newblock On fast leverage score sampling and optimal learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5672--5682, 2018.

\bibitem{saunders98_ridge_regres_learn_algor_dual_variab}
C.~Saunders, A.~Gammerman, and V.~Vovk.
\newblock Ridge regression learning algorithm in dual variables.
\newblock 1998.

\bibitem{settles12_activ_learn}
B.~Settles.
\newblock Active learning.
\newblock {\em Synthesis Lectures on Artificial Intelligence and Machine
  Learning}, 6(1):1--114, Jun 2012.

\bibitem{settles08}
B.~Settles and M.~Craven.
\newblock An analysis of active learning strategies for sequence labeling
  tasks.
\newblock In {\em Proceedings of the conference on empirical methods in natural
  language processing}, pages 1070--1079. Association for Computational
  Linguistics, 2008.

\bibitem{shalev-shwartz14_under}
S.~Shalev-Shwartz and S.~Ben-David.
\newblock {\em Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem{smale07_learn_theor_estim_via_integ}
S.~Smale and D.-X. Zhou.
\newblock Learning theory estimates via integral operators and their
  approximations.
\newblock {\em Constructive approximation}, 26(2):153--172, 2007.

\bibitem{tolstikhin17_minim_estim_kernel_mean_embed}
I.~Tolstikhin, B.~K. Sriperumbudur, and K.~Muandet.
\newblock Minimax estimation of kernel mean embeddings.
\newblock {\em The Journal of Machine Learning Research}, 18(1):3002--3048,
  2017.

\bibitem{tong01_suppor_vector_machin_activ_learn}
S.~Tong and D.~Koller.
\newblock Support vector machine active learning with applications to text
  classification.
\newblock {\em Journal of machine learning research}, 2(Nov):45--66, 2001.

\bibitem{vapnik92_princ}
V.~Vapnik.
\newblock Principles of risk minimization for learning theory.
\newblock In {\em Advances in neural information processing systems}, pages
  831--838, 1992.

\bibitem{viering17_nuclear_discr_activ_learn}
T.~J. Viering, J.~H. Krijthe, and M.~Loog.
\newblock Nuclear discrepancy for active learning, 2017.

\bibitem{welling09_herdin}
M.~Welling.
\newblock Herding dynamical weights to learn.
\newblock In {\em Proceedings of the 26th Annual International Conference on
  Machine Learning}, pages 1121--1128. ACM, 2009.

\bibitem{willett06_faster_rates_regres_activ_learn}
R.~Willett, R.~Nowak, and R.~M. Castro.
\newblock Faster rates in regression via active learning.
\newblock In Y.~Weiss, B.~Sch\"{o}lkopf, and J.~C. Platt, editors, {\em
  Advances in Neural Information Processing Systems 18}, pages 179--186. MIT
  Press, 2006.

\bibitem{williams96_gauss}
C.~K. Williams and C.~E. Rasmussen.
\newblock Gaussian processes for regression.
\newblock In {\em Advances in neural information processing systems}, pages
  514--520, 1996.

\bibitem{wu18_pool_based_sequen_activ_learn_regres}
D.~Wu.
\newblock Pool-based sequential active learning for regression.
\newblock {\em IEEE transactions on neural networks and learning systems},
  30(5):1348--1359, 2018.

\bibitem{xu19_towar_effic_evaluat_risk_via_herdin}
Z.~Xu, T.~Yu, and S.~Sra.
\newblock Towards efficient evaluation of risk via herding.
\newblock 2019.

\bibitem{zhu03_combin}
X.~Zhu, J.~Lafferty, and Z.~Ghahramani.
\newblock Combining active learning and semi-supervised learning using gaussian
  fields and harmonic functions.
\newblock In {\em ICML 2003 workshop on the continuum from labeled to unlabeled
  data in machine learning and data mining}, volume~3, 2003.

\bibitem{zwald06}
L.~Zwald and G.~Blanchard.
\newblock On the convergence of eigenspaces in kernel principal component
  analysis.
\newblock In {\em Advances in neural information processing systems}, pages
  1649--1656, 2006.

\end{thebibliography}
