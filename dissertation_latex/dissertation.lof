\babel@toc {english}{}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Learning curve (bold line is mean with shaded region being \(\pm 1\) standard deviation over the 5 folds) for agnostic regression on Boston dataset comparing FW (KH) with MC and Levscore. The \(y\)-axis is MSE (Mean Squared Error), \(x\)-axis is \(t\) (number of datapoints in current active learning train set). A good algorithm will have a trajectory that goes down quickly with \(t\). MC is baseline.\relax }}{46}{figure.caption.8}% 
\contentsline {figure}{\numberline {4.2}{\ignorespaces Learning curve (bold line is mean with shaded region being \(\pm 1\) standard deviation over the 5 folds) for realisable regression on Boston dataset comparing FW (KH) with MC and Levscore. The \(y\)-axis is MSE (Mean Squared Error), \(x\)-axis is \(t\) (number of datapoints in current active learning train set). A good algorithm will have a trajectory that goes down quickly with \(t\). MC is baseline.\relax }}{47}{figure.caption.9}% 
\contentsline {figure}{\numberline {4.3}{\ignorespaces Learning curve (bold line is mean with shaded region being \(\pm 1\) standard deviation over the 5 folds) for agnostic classification on mnist comparing FW (KH) with MC and Levscore. The \(y\)-axis is Accuracy, \(x\)-axis is \(t\) (number of datapoints in current active learning train set). A good algorithm will have a trajectory that goes up quickly with \(t\). MC is baseline.\relax }}{48}{figure.caption.10}% 
\contentsline {figure}{\numberline {4.4}{\ignorespaces Comparison of the first 9 instances chosen by FW (KH), MC and Levscore. The data was generated by sampling 50 datapoints from 9 gaussian distributions with means from the set \(\{(i, j)| i, j \in \{-1, 0, 1\}\}\) and covariance matrix being the identity matrix scaled by 0.01. The kernel matrix was created by using a gaussian kernel with \(\sigma ^{2} = 0.02\) which was fine-tuned to exhibit this phenomenon.\relax }}{48}{figure.caption.11}% 
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Learning curves (bold line is mean with shaded region being \(\pm 1\) standard deviation over the 5 folds) for agnostic regression comparing FW (KH) with MC and Levscore. The \(y\)-axis is MSE (Mean Squared Error), \(x\)-axis is \(t\) (number of datapoints in current active learning train set). A good algorithm will have a trajectory that goes down quickly with \(t\). MC is baseline.\relax }}{54}{figure.caption.12}% 
\contentsline {figure}{\numberline {6.2}{\ignorespaces Learning curves (bold line is mean with shaded region being \(\pm 1\) standard deviation over the 5 folds) for regression comparing FW (KH) with MC and Levscore. The \(y\)-axis is MSE (Mean Squared Error), \(x\)-axis is \(t\) (number of datapoints in current active learning train set). A good algorithm will have a trajectory that goes down quickly with \(t\). MC is baseline.\relax }}{55}{figure.caption.13}% 
\contentsline {figure}{\numberline {6.3}{\ignorespaces Learning curves (bold line is mean with shaded region being \(\pm 1\) standard deviation over the 5 folds) for agnostic classification comparing FW (KH) with MC and Levscore. The \(y\)-axis is Accuracy, \(x\)-axis is \(t\) (number of datapoints in current active learning train set). A good algorithm will have a trajectory that goes up quickly with \(t\). MC is baseline.\relax }}{56}{figure.caption.14}% 
